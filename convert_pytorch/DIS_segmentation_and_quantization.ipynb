{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWoqui4egB0q"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 The AI Edge Torch Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvt-8e8eE1da"
      },
      "source": [
        "This demo will teach you how to convert a PyTorch [IS-Net](https://github.com/xuebinqin/DIS) model to a LiteRT model using Google's AI Edge Torch library. You will then run the newly converted `tflite` model locally using the LiteRT API, as well as learn where to find other tools for running your newly converted model on other edge hardware, including mobile devices and web browsers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzf2MdHoG-9c"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hux_Gsc_G4nl"
      },
      "source": [
        "You can start by importing the necessary dependencies for converting the model, as well as some additional utilities for displaying various information as you progress through this sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-9--DWON236"
      },
      "outputs": [],
      "source": [
        "!pip install ai-edge-torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUMh9GRk17fV"
      },
      "source": [
        "You will also need to download an image to verify model functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TDCmXEplIyB"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['astrid_happy_hike.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "  url = f'https://storage.googleapis.com/ai-edge/models-samples/torch_converter/image_segmentation_dis/{name}'\n",
        "  urllib.request.urlretrieve(url, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below. Additionally, this will allow you to select multiple images to upload and test at each step in this colab."
      ],
      "metadata": {
        "id": "SDb92B05EZi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded:\n",
        "  content = uploaded[filename]\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(content)\n",
        "IMAGE_FILENAMES = list(uploaded.keys())\n",
        "\n",
        "print('Uploaded files:', IMAGE_FILENAMES)"
      ],
      "metadata": {
        "id": "rPDSVb3oEb19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now go ahead and verify that the image was loaded successfully"
      ],
      "metadata": {
        "id": "mgc88K-CEeb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "\n",
        "# Preview the images.\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ],
      "metadata": {
        "id": "D754sE6WEhOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we've written a few utility functions to help with visualizing each step in this process, as well as one function that performs inference using the various models that can be passed into it. Go ahead and run this cell now so that they're available."
      ],
      "metadata": {
        "id": "bqmpr-BI3tAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def display_two_column_images(title_1, title_2, image_1, image_2):\n",
        "    f, ax = plt.subplots(1, 2, figsize = (7,7))\n",
        "    ax[0].imshow(image_1)\n",
        "    ax[1].imshow(image_2, cmap = 'gray')\n",
        "    ax[0].set_title(title_1)\n",
        "    ax[1].set_title(title_2)\n",
        "    ax[0].axis('off')\n",
        "    ax[1].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  def display_three_column_images(title_1, title_2, title_3, image_1, image_2, image_3):\n",
        "    f, ax = plt.subplots(1, 3, figsize = (10,10))\n",
        "    ax[0].imshow(image_1)  # Original image.\n",
        "    ax[1].imshow(image_2, cmap = 'gray')  # PT segmentation mask.\n",
        "    ax[2].imshow(image_3, cmap = 'gray')  # TFL segmentation mask.\n",
        "    ax[0].set_title(title_1)\n",
        "    ax[1].set_title(title_2)\n",
        "    ax[2].set_title(title_3)\n",
        "    ax[0].axis('off')\n",
        "    ax[1].axis('off')\n",
        "    ax[2].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  def get_processed_isnet_result(model_output, original_image_hw):\n",
        "    # Min-max normalization.\n",
        "    output_min = model_output.min()\n",
        "    output_max = model_output.max()\n",
        "    result = (model_output - output_min) / (output_max - output_min)\n",
        "\n",
        "    # Scale [0, 1] -> [0, 255].\n",
        "    result = (result * 255).astype(np.uint8)\n",
        "\n",
        "    # Restore original image size.\n",
        "    result = Image.fromarray(result.squeeze(), \"L\")\n",
        "    return result.resize(original_image_hw, Image.Resampling.BILINEAR)"
      ],
      "metadata": {
        "id": "fzqBAJtU3sb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFYQIm-yFz1"
      },
      "source": [
        "# PyTorch model validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKfMAS7ggB0w"
      },
      "source": [
        "Now that you have your test images and utility functions, it's time to test the original PyTorch model that will be converted to the `tflite` format. You can start by retrieving the PyTorch model from Kaggle, along with the original project from GitHub that will be used for building the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywS-73O6gB0x"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf DIS sample_data\n",
        "\n",
        "!git clone https://github.com/xuebinqin/DIS.git\n",
        "%cd DIS/IS-Net/\n",
        "\n",
        "!curl -o ./model.tar.gz -L https://www.kaggle.com/api/v1/models/paulruiz/dis/pyTorch/8-17-22/1/download\n",
        "!tar -xvf 'model.tar.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW3TdIhyr-ds"
      },
      "source": [
        "Next you will load in that new model build it to run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvyEsyNQp7FT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from models import ISNetDIS\n",
        "\n",
        "pytorch_model_filename = 'isnet-general-use.pth'\n",
        "pt_model = ISNetDIS()\n",
        "pt_model.load_state_dict(\n",
        "    torch.load(pytorch_model_filename, map_location=torch.device('cpu'))\n",
        ")\n",
        "pt_model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5d4s8SSr8wn"
      },
      "source": [
        "And to finish validating the original model, you can use it to run inference on the test image(s) that you loaded earlier. In this step you will save the generated PyTorch segmentation mask images so they can be compared to your LiteRT segmentation mask images later in this colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XefR4a2nGqmz"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import normalize\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "MODEL_INPUT_HW = (1024, 1024)\n",
        "pt_result = []\n",
        "images = []\n",
        "for index in range(len(IMAGE_FILENAMES)) :\n",
        "  images.append(io.imread('../../'+IMAGE_FILENAMES[index]))\n",
        "\n",
        "  # BHWC -> BCHW.\n",
        "  image_tensor = torch.tensor(images[index], dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "  # Resize to meet model input size requirements.\n",
        "  image_tensor = F.upsample(torch.unsqueeze(image_tensor, 0),\n",
        "                            MODEL_INPUT_HW, mode='bilinear').type(torch.uint8)\n",
        "\n",
        "  # Scale [0, 255] -> [0, 1].\n",
        "  pt_image = torch.divide(image_tensor, 255.0)\n",
        "\n",
        "  # Normalize.\n",
        "  pt_image = normalize(pt_image, mean=[0.5, 0.5, 0.5], std=[1.0, 1.0, 1.0])\n",
        "\n",
        "  # Get output with the most accurate prediction.\n",
        "  pt_result.append(pt_model(pt_image)[0][0])\n",
        "\n",
        "  # Recover the prediction spatial size to the orignal image size.\n",
        "  pt_result[index] = F.upsample(pt_result[index], images[index].shape[:2],  mode='bilinear')\n",
        "  pt_result[index] = torch.squeeze(pt_result[index], 0)\n",
        "\n",
        "  # Min-max normalization.\n",
        "  ma = torch.max(pt_result[index])\n",
        "  mi = torch.min(pt_result[index])\n",
        "  pt_result[index] = (pt_result[index] - mi) / (ma - mi)\n",
        "\n",
        "  # Scale [0, 1] -> [0, 255].\n",
        "  pt_result[index] = pt_result[index] * 255\n",
        "\n",
        "  # BCHW -> BHWC.\n",
        "  pt_result[index] = pt_result[index].permute(1, 2, 0)\n",
        "\n",
        "  # Get numpy array.\n",
        "  pt_result[index] = pt_result[index].cpu().data.numpy().astype(np.uint8)\n",
        "\n",
        "  display_two_column_images('Original Image', 'Mask', images[index], pt_result[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to LiteRT"
      ],
      "metadata": {
        "id": "2v1rYRI68Jdl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7zWa2S7eLU"
      },
      "source": [
        "## Add model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHO3K-kXXHWp"
      },
      "source": [
        "The original IS-Net model generates 12 outputs, each corresponding to different stages in the segmentation process. While the official PyTorch model demo provides guidance on selecting the final (best) output, obtaining the desired output from the converted LiteRT model requires additional effort.\n",
        "\n",
        "One of the methods you can use to get to this final output is to download the `tflite` file after the conversion step in this colab, open it with [Model Explorer](https://ai.google.dev/edge/model-explorer) and confirm which output in the graph has the expected output shape.\n",
        "\n",
        "That's kind of a lot for this example, so to simplify the process and eliminate this effort, you can use a wrapper for the PyTorch model that narrows the scope to only the final output. This approach ensures that your new LiteRT model has only a single output after the conversion stage.\n",
        "\n",
        "Additionally, this colab include some extra pre and post-processing steps, such as excluding min-max normalization because `torch.min` and `torch.max` are not currently supported in the conversion process.\n",
        "\n",
        "You can create the wrapper by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr2XESVJGucI"
      },
      "outputs": [],
      "source": [
        "class ImageSegmentationModelWrapper(nn.Module):\n",
        "\n",
        "  RESCALING_FACTOR = 255.0\n",
        "  MEAN = 0.5\n",
        "  STD = 1.0\n",
        "\n",
        "  def __init__(self, pt_model):\n",
        "    super().__init__()\n",
        "    self.model = pt_model\n",
        "\n",
        "  def forward(self, image: torch.Tensor):\n",
        "    # BHWC -> BCHW.\n",
        "    image = image.permute(0, 3, 1, 2)\n",
        "\n",
        "    # Rescale [0, 255] -> [0, 1].\n",
        "    image = image / self.RESCALING_FACTOR\n",
        "\n",
        "    # Normalize.\n",
        "    image = (image - self.MEAN) / self.STD\n",
        "\n",
        "    # Get result.\n",
        "    result = self.model(image)[0][0]\n",
        "\n",
        "    # BHWC -> BCHW.\n",
        "    result = result.permute(0, 2, 3, 1)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "wrapped_pt_model = ImageSegmentationModelWrapper(pt_model).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMBNfgcV7k0f"
      },
      "source": [
        "## Convert to LiteRT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2MnULes70W0"
      },
      "source": [
        "Provide sample arguments -- result LiteRT model will expect input of this size -- and convert the model.\n",
        "\n",
        "Now it's time to perform the conversion! You will need to provide a couple arguments, such as the expected input shape (for example: 1, model input height, model input width, and 3 for the RGB layers of an image) and the wrapper that you created in the last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOfNPYpnLGrp"
      },
      "outputs": [],
      "source": [
        "import ai_edge_torch\n",
        "\n",
        "sample_args = (torch.rand((1, *MODEL_INPUT_HW, 3)),)\n",
        "edge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7II2a_389DH"
      },
      "source": [
        "# Validate converted model with LiteRT Interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F65_ULYRLkTY"
      },
      "source": [
        "Now that you have a converted model stored in colab, it's time to test it. You can start by preparing the test image(s) that you loaded earlier. Since all of the preprocessing steps were into the model earlier, you will only need to resize and type cast the input image(s) in this step. At the end of this stage you should see the original image, the PyTorch mask graphic, and the LiteRT mask graphic for your test input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQBmo3uqMC8p"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "np_images = []\n",
        "image_sizes = []\n",
        "for index in range(len(IMAGE_FILENAMES)) :\n",
        "  # Retrieve each image from the file system\n",
        "  image = Image.open('../../' + IMAGE_FILENAMES[index])\n",
        "  # Track each image's size here to simplify displaying later\n",
        "  image_sizes.append(image.size)\n",
        "  # Convert each image into a NumPy array and save for later\n",
        "  np_images.append(np.array(image.resize(MODEL_INPUT_HW, Image.Resampling.BILINEAR)))\n",
        "  np_images[index] = np.expand_dims(np_images[index], axis=0).astype(np.float32)\n",
        "\n",
        "  # Retrieve an output from the converted model\n",
        "  edge_model_output = edge_model(np_images[index])\n",
        "\n",
        "  # Use the visualization utility created earlier to get a displayable image\n",
        "  lrt_result = get_processed_isnet_result(edge_model_output, image_sizes[index])\n",
        "\n",
        "  display_three_column_images('Original Image', 'PT Mask', 'TFL Mask', images[index], pt_result[index], lrt_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVcrUu9aaP9W"
      },
      "source": [
        "# Post Training and Dynamic-Range Quantization with LiteRT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point you should have a working `tflite` model that you have converted from the original PyTorch format. Congratulations! But if you're working with edge devices, then you likely know that model size is an **important** consideration for things like mobile devices. Using a technique called *quantization*, you can reduce a model's size to roughly a quarter of the original size while maintaining a similar level of output quality. To do this with the Google AI Edge PyTorch Converter, you can pass in an optimization flag to the `convert` function to include a step for dynamic-range quantization.\n",
        "\n",
        "If you'd like to know more about quantization and other optimizations, you can find our official documentation [here](https://www.tensorflow.org/lite/performance/post_training_quantization)."
      ],
      "metadata": {
        "id": "dKQilmWqnzpV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDmkx7zLaXn8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tfl_converter_flags={\n",
        "    \"optimizations\": [tf.lite.Optimize.DEFAULT]\n",
        "}\n",
        "tfl_drq_model = ai_edge_torch.convert(\n",
        "    wrapped_pt_model,\n",
        "    sample_args,\n",
        "    _ai_edge_converter_flags=tfl_converter_flags\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the conversion has finished, you can compare the newly converted and quantized model with the original image and PyTorch mask image from earlier."
      ],
      "metadata": {
        "id": "y-Ty1YsvozQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fJtHyxbaejb"
      },
      "outputs": [],
      "source": [
        "for index in range(len(IMAGE_FILENAMES)) :\n",
        "\n",
        "  tfl_drq_model_output = tfl_drq_model(np_images[index])\n",
        "\n",
        "  tfl_drq_result = get_processed_isnet_result(tfl_drq_model_output, image_sizes[index])\n",
        "\n",
        "  display_three_column_images('Original Image', 'PT Mask', 'TFLQ Mask', images[index], pt_result[index], tfl_drq_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Training and Dynamic-Range Quantization with PT2E"
      ],
      "metadata": {
        "id": "me3y_PzayhyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another available option for dynamic-range quantization is called PT2E, which is a framework-level quantization feature available in PyTorch 2.0. For more details see [PyTorch tutorial](https://pytorch.org/tutorials/prototype/quantization_in_pytorch_2_0_export_tutorial.html).\n",
        "\n",
        "PT2EQuantizer is developed specifically for the AI Edge Torch framework and is configured to quantize models leveraging various operators and kernals offered by the LiteRT Runtime.\n",
        "\n",
        "You can see how to configure the PT2EQuantizer and use it as an additional parameter in the `convert` function below."
      ],
      "metadata": {
        "id": "Xv6hZkvqmdHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ai_edge_torch.quantize.pt2e_quantizer import get_symmetric_quantization_config\n",
        "from ai_edge_torch.quantize.pt2e_quantizer import PT2EQuantizer\n",
        "from ai_edge_torch.quantize.quant_config import QuantConfig\n",
        "\n",
        "from torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\n",
        "from torch._export import capture_pre_autograd_graph\n",
        "\n",
        "\n",
        "pt2e_quantizer = PT2EQuantizer().set_global(\n",
        "    get_symmetric_quantization_config(is_per_channel=True, is_dynamic=True)\n",
        ")\n",
        "\n",
        "# Following are the required steps recommended in the PT2E quantization\n",
        "# workflow.\n",
        "autograd_torch_model = capture_pre_autograd_graph(wrapped_pt_model, sample_args)\n",
        "# 1. Prepare for quantization.\n",
        "pt2e_torch_model = prepare_pt2e(autograd_torch_model, pt2e_quantizer)\n",
        "# 2. Run the prepared model with sample input data to ensure that internal\n",
        "# observers are populated with correct values.\n",
        "pt2e_torch_model(*sample_args)\n",
        "# 3. Finally, convert (quantize) the prepared model.\n",
        "pt2e_torch_model = convert_pt2e(pt2e_torch_model, fold_quantize=False)\n",
        "\n",
        "pt2e_drq_model = ai_edge_torch.convert(\n",
        "    pt2e_torch_model,\n",
        "    sample_args,\n",
        "    quant_config=QuantConfig(pt2e_quantizer=pt2e_quantizer)\n",
        ")"
      ],
      "metadata": {
        "id": "-AjWfYAoy3D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has been converted again using the PT2E Quantizer, it's time to review the results so you can compare them to both the original image and the PyTorch inferred mask."
      ],
      "metadata": {
        "id": "ZxQDTvQZnlp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(len(IMAGE_FILENAMES)) :\n",
        "\n",
        "  pt2e_drq_output = pt2e_drq_model(np_images[index])\n",
        "\n",
        "  pt2e_drq_result = get_processed_isnet_result(pt2e_drq_output, image_sizes[index])\n",
        "\n",
        "  display_three_column_images('Original Image', 'PT Mask', 'PT2E DRQ Mask', images[index], pt_result[index], pt2e_drq_result)"
      ],
      "metadata": {
        "id": "PipB5Og-0dx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AOmkXUaBVUb"
      },
      "source": [
        "# Download converted models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've converted and optimized the DIS model for LiteRT, it's time to save those models. The following cells are set up to download three models: the newly converted `tflite` model without optimizations, the converted model using dynamic range quantization, and the model that uses PT2E quantization. When you've finished downloading these files, check out their finished sizes! You'll notice that the original converted model is about 175MB in size, whereas the quantized models are about 45MB - much more manageable for edge devices!"
      ],
      "metadata": {
        "id": "TetqIwvEnu__"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY00XJQ1BZP3"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "tfl_filename = \"isnet.tflite\"\n",
        "edge_model.export(tfl_filename)\n",
        "\n",
        "files.download(tfl_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgFa0lDSd7Z5"
      },
      "outputs": [],
      "source": [
        "tfl_drq_filename = 'isnet_tfl_drq.tflite'\n",
        "tfl_drq_model.export(tfl_drq_filename)\n",
        "\n",
        "files.download(tfl_drq_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt2e_drq_filename = 'isnet_pt2e_drq.tflite'\n",
        "pt2e_drq_model.export(pt2e_drq_filename)\n",
        "\n",
        "files.download(pt2e_drq_filename)"
      ],
      "metadata": {
        "id": "-NGABbj-0hiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps\n",
        "\n",
        "Now that you have learned how to convert this segmentation model from the PyTorch to tflite format, it's time to do more with it! You can go over additional LiteRT API samples for multiple platforms, including Android, iOS, and Python, as well as learn more about on-device machine learning inference from the [Google AI Edge official documentation](https://ai.google.dev/edge/). You can find samples to run the output from this colab on Android [here](https://github.com/google-ai-edge/litert-samples/tree/main/examples/image_segmentation_DIS/android) and on iOS [here](https://github.com/google-ai-edge/litert-samples/tree/main/examples/image_segmentation_DIS/ios)."
      ],
      "metadata": {
        "id": "t40oY48RogjW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
