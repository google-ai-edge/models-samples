{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln-0JGdPeeyz"
      },
      "source": [
        "# Convert a SegNeXt PyTorch Model to LiteRT\n",
        "\n",
        "This notebook demonstrates how to convert a **SegNeXt** model (originally trained and published in PyTorch) into a LiteRT model using [AI Edge Torch](https://ai.google.dev/edge). The sample also shows how to optimize the resulting model with dynamic-range quantization using [AI Edge Quantizer](https://github.com/google-ai-edge/ai-edge-quantizer).\n",
        "\n",
        "## What you'll learn\n",
        "\n",
        "- **Setup:** Installing necessary libraries and tools to download and load the SegNeXt model.\n",
        "- **Inference Validation:** Running the PyTorch model for segmentation.\n",
        "- **Model Conversion:** Converting a SegNext model to LiteRT using AI Edge Torch.\n",
        "- **Verifying Results:** Comparing outputs between PyTorch and LiteRT models.\n",
        "- **Quantization:** Applying post-training quantization techniques to reduce model size.\n",
        "- **Export and Download**: Download your newly created or optimized LiteRT model."
      ],
      "id": "ln-0JGdPeeyz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Y9CioSeey1"
      },
      "source": [
        "## Install and Import Dependencies\n",
        "\n",
        "You can start by importing the necessary dependencies for converting the model, as well as some additional tweaks to get the `mmsegmentation` library working as expected with the AI Torch Edge Converter.\n",
        "\n",
        "Make sure to run the following cells to set up the environment with the required libraries:"
      ],
      "id": "04Y9CioSeey1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "installation_code",
        "language_info": {
          "name": "python"
        }
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install MMCV and its dependencies.\n",
        "!pip install openmim -q\n",
        "!mim install mmengine -q\n",
        "!mim install mmcv==2.2.0 -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.4/index.html\n",
        "!pip install ftfy\n",
        "\n",
        "# Install AI Torch Edge and Quantizer.\n",
        "!pip install ai-edge-torch-nightly -q\n",
        "!pip install ai-edge-quantizer-nightly -q"
      ],
      "id": "installation_code"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the MMSegmentation GitHub repository.\n",
        "!git clone -b v1.2.2 https://github.com/open-mmlab/mmsegmentation.git\n",
        "\n",
        "# Patch the version constraints in mmseg/__init__.py\n",
        "!sed -i \"s/MMCV_MAX = '2.2.0'/MMCV_MAX = '6.5.0'/g\" mmsegmentation/mmseg/__init__.py\n",
        "\n",
        "# Install MMSegmentation\n",
        "%cd mmsegmentation\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "u1AlddWxioAh"
      },
      "id": "u1AlddWxioAh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import cv2\n",
        "import math\n",
        "import sys\n",
        "\n",
        "# PyTorch, Vision, and AI Edge Torch.\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import ai_edge_torch\n",
        "\n",
        "# PIL, NumPy, IPython display.\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "\n",
        "# Google Colab utilities.\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Matplotlib for visualization.\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# AI Edge Torch Quantization utilities.\n",
        "import ai_edge_litert\n",
        "from ai_edge_quantizer import quantizer, recipe"
      ],
      "metadata": {
        "id": "vxicg9FaiWrC"
      },
      "id": "vxicg9FaiWrC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patch the `MMEngine` registry\n",
        "We'll also patch the `Registry` to address potential naming collisions in the mmseg registry, then import our classes and create an inference object."
      ],
      "metadata": {
        "id": "NWIrsDX2ipCq"
      },
      "id": "NWIrsDX2ipCq"
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown We implemented some functions to patch the mmengine registry. <br/> Run the following cell to activate the functions.\n",
        "%%writefile patch_registry.py\n",
        "import logging\n",
        "\n",
        "from mmengine.registry import Registry\n",
        "from mmengine.logging import print_log\n",
        "from typing import Type, Optional, Union, List\n",
        "\n",
        "def _register_module(self,\n",
        "                     module: Type,\n",
        "                     module_name: Optional[Union[str, List[str]]] = None,\n",
        "                     force: bool = False) -> None:\n",
        "    \"\"\"Register a module.\n",
        "\n",
        "    Args:\n",
        "        module (type): Module to be registered. Typically a class or a\n",
        "            function, but generally all ``Callable`` are acceptable.\n",
        "        module_name (str or list of str, optional): The module name to be\n",
        "            registered. If not specified, the class name will be used.\n",
        "            Defaults to None.\n",
        "        force (bool): Whether to override an existing class with the same\n",
        "            name. Defaults to False.\n",
        "    \"\"\"\n",
        "    if not callable(module):\n",
        "        raise TypeError(f'module must be Callable, but got {type(module)}')\n",
        "\n",
        "    if module_name is None:\n",
        "        module_name = module.__name__\n",
        "    if isinstance(module_name, str):\n",
        "        module_name = [module_name]\n",
        "    for name in module_name:\n",
        "        if not force and name in self._module_dict:\n",
        "            existed_module = self.module_dict[name]\n",
        "            print_log(\n",
        "                f'{name} is already registered in {self.name} '\n",
        "                f'at {existed_module.__module__}. Registration ignored.',\n",
        "                logger='current',\n",
        "                level=logging.INFO\n",
        "            )\n",
        "        self._module_dict[name] = module\n",
        "\n",
        "Registry._register_module = _register_module\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vQ6zJOsniihd"
      },
      "id": "vQ6zJOsniihd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch the MMEngine registry.\n",
        "import patch_registry\n",
        "\n",
        "# Check MMSegmentation installation.\n",
        "import mmseg\n",
        "print(mmseg.__version__)\n",
        "\n",
        "# Import the `apis` and `datasets` modules.\n",
        "from mmseg import apis, datasets"
      ],
      "metadata": {
        "id": "XZ73z0fYi1l7"
      },
      "id": "XZ73z0fYi1l7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp0ZAmzYeey2"
      },
      "source": [
        "### Download a Sample Image\n",
        "We'll retrieve an image that we'll use for our segmentation demo. Feel free to upload your own image(s) if desired."
      ],
      "id": "hp0ZAmzYeey2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_image"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['test.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "    url = 'https://upload.wikimedia.org/wikipedia/commons/9/9c/Bruce_car.JPG'\n",
        "    urllib.request.urlretrieve(url, name)"
      ],
      "id": "download_image"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnZkAy-Peey2"
      },
      "source": [
        "If you want to upload additional images, uncomment and run the cell below. Then update `IMAGE_FILENAMES` to match your uploaded file(s)."
      ],
      "id": "JnZkAy-Peey2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upload_images_example"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "#\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "#\n",
        "# IMAGE_FILENAMES = list(uploaded.keys())\n",
        "# print('Uploaded files:', IMAGE_FILENAMES)"
      ],
      "id": "upload_images_example"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYPROgSgeey3"
      },
      "source": [
        "Quickly display the loaded image(s) to confirm."
      ],
      "id": "nYPROgSgeey3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "verify_images"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "def resize_and_show(image):\n",
        "    h, w = image.shape[:2]\n",
        "    if h < w:\n",
        "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "    else:\n",
        "        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "    cv2_imshow(img)\n",
        "\n",
        "# Preview the images.\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "\n",
        "for name, image in images.items():\n",
        "    print(name)\n",
        "    resize_and_show(image)"
      ],
      "id": "verify_images"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMJ7KVrUeey3"
      },
      "source": [
        "## Load SegNext\n",
        "We'll clone the [mmsegmentation](https://github.com/open-mmlab/mmsegmentation) repo, install it, and then load a SegNeXt model trained on [ADE20K](https://ade20k.csail.mit.edu/). In this example, we're using the [SegNeXt mscan-b ADE20K model](https://github.com/open-mmlab/mmsegmentation/tree/main/configs/segnext)."
      ],
      "id": "fMJ7KVrUeey3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports_mmseg"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the SegNext PyTorch model while setting the device to CPU.\n",
        "inferencer = apis.MMSegInferencer(model='segnext_mscan-b_1xb16-adamw-160k_ade20k-512x512', device='cpu')\n",
        "\n",
        "# Retrieve the actual PyTorch model.\n",
        "pt_model = inferencer.model\n",
        "pt_model.eval()"
      ],
      "id": "imports_mmseg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The MIT ADE20K scene parsing dataset  \n",
        "`ADE20K` is composed of more than 27K images from the [SUN](https://groups.csail.mit.edu/vision/SUN/hierarchy.html) and [Places](https://www.csail.mit.edu/research/places-database-scene-recognition) databases. Images are fully annotated with objects, spanning over 3K object categories."
      ],
      "metadata": {
        "id": "Kh2jlhm33EcH"
      },
      "id": "Kh2jlhm33EcH"
    },
    {
      "cell_type": "code",
      "source": [
        "classes = datasets.ADE20KDataset.METAINFO['classes']\n",
        "palette = datasets.ADE20KDataset.METAINFO['palette']"
      ],
      "metadata": {
        "id": "iUgM8mNh3Wa5"
      },
      "id": "iUgM8mNh3Wa5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the dataset, we extract the class labels and the color palette from its metadata. We also retrieve the mean and standard deviation values from the data preprocessor configuration (via `inferencer.cfg`), which will be essential when using the converter later."
      ],
      "metadata": {
        "id": "9T8phj8m16Lj"
      },
      "id": "9T8phj8m16Lj"
    },
    {
      "cell_type": "code",
      "source": [
        "data_preprocessor_dict = inferencer.cfg.to_dict()['data_preprocessor']\n",
        "data_preprocessor_dict['mean'], data_preprocessor_dict['std']"
      ],
      "metadata": {
        "id": "q4ChjHSm1-NM"
      },
      "id": "q4ChjHSm1-NM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fQSgQFveey4"
      },
      "source": [
        "## Inference using the PyTorch Model\n",
        "Let's verify the PyTorch model by doing a quick inference and visualizing the segmentation output.\n"
      ],
      "id": "_fQSgQFveey4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_pt_model"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# The output mask is saved under 'outputs/vis/test.jpg'\n",
        "results = inferencer(IMAGE_FILENAMES[0], out_dir='outputs', img_out_dir='vis', return_vis=True)"
      ],
      "id": "test_pt_model"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the segmentation result."
      ],
      "metadata": {
        "id": "gPmjDctI4Gds"
      },
      "id": "gPmjDctI4Gds"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "display_mask"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "display.Image('outputs/vis/test.jpg')"
      ],
      "id": "display_mask"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now confirmed that the original PyTorch model can generate valid segmentation predictions and that it runs properly in Python."
      ],
      "metadata": {
        "id": "rdcp23wEe-1w"
      },
      "id": "rdcp23wEe-1w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD_DyvBheey4"
      },
      "source": [
        "\n",
        "## Create a Model Wrapper\n",
        "To simplify the model output and ensure a single output node during conversion, we'll create a wrapper module. We'll also handle the typical mean/std normalization manually within this wrapper (since some methods, like `torch.min` or `torch.max`, might not be fully supported in the LiteRT conversion)."
      ],
      "id": "aD_DyvBheey4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_wrapper"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ImageSegmentationModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, pt_model, mmseg_cfg):\n",
        "        super().__init__()\n",
        "        self.model = pt_model\n",
        "        data_preprocessor_dict = mmseg_cfg.to_dict()['data_preprocessor']\n",
        "        # Convert the mean and std from shape (3,) to (1, 3, 1, 1)\n",
        "        self.image_mean = torch.tensor(data_preprocessor_dict['mean']).view(1, -1, 1, 1)\n",
        "        self.image_std = torch.tensor(data_preprocessor_dict['std']).view(1, -1, 1, 1)\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        # Input shape: (N, H, W, C)\n",
        "        # Convert BHWC to BCHW.\n",
        "        image = image.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Normalize.\n",
        "        image = (image - self.image_mean) / self.image_std\n",
        "\n",
        "        # Model output is typically (N, C, H, W).\n",
        "        result = self.model(image)\n",
        "\n",
        "        # Convert from NCHW to NHWC.\n",
        "        result = result.permute(0, 2, 3, 1)\n",
        "\n",
        "        return result\n",
        "\n",
        "# Create the wrapped model.\n",
        "wrapped_pt_model = ImageSegmentationModelWrapper(pt_model, inferencer.cfg).eval()"
      ],
      "id": "create_wrapper"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmcvaKlheey5"
      },
      "source": [
        "## Convert to LiteRT\n",
        "\n",
        "One of the methods you can use to get to this final output is to download the `tflite` file after the conversion step in this colab, open it with [Model Explorer](https://ai.google.dev/edge/model-explorer) and confirm which output in the graph has the expected output shape.\n",
        "\n",
        "That's kind of a lot for this example, so to simplify the process and eliminate this effort, you can use a wrapper for the PyTorch model that narrows the scope to only the final output. This approach ensures that your new LiteRT model has only a single output after the conversion stage.\n",
        "\n",
        "We'll use AI Edge Torch to convert our PyTorch model. We pass in a sample input of appropriate shape to guide the conversion. (This shape also becomes your expected inference shape in the resulting LiteRT model.)"
      ],
      "id": "LmcvaKlheey5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "convert_to_litert"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "MODEL_INPUT_HW = (512, 512)\n",
        "sample_args = (torch.rand((1, *MODEL_INPUT_HW, 3)),)\n",
        "\n",
        "edge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args)"
      ],
      "id": "convert_to_litert"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC-HGLCoeey5"
      },
      "source": [
        "## Validate Converted Model with LiteRT Interpreter\n",
        "We can test the converted LiteRT model's output. Since our pre-processing is embedded within the wrapper, we'll only resize and cast the input image.\n"
      ],
      "id": "jC-HGLCoeey5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vis_utils",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @markdown We implemented some functions to visualize the segmentation results. <br/> Run the following cell to activate the functions.\n",
        "\n",
        "# Visualization utilities\n",
        "def label_to_color_image(label, palette):\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "    colormap = np.asarray(palette)\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "    return colormap[label]\n",
        "\n",
        "def vis_segmentation(image, seg_map, palette, label_names):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map, palette).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    H, W = image.shape[:2]\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image, extent=(0, W, H, 0))\n",
        "    plt.imshow(seg_image, alpha=0.7, extent=(0, W, H, 0))\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    full_color_map = label_to_color_image(\n",
        "        np.arange(len(label_names)).reshape(len(label_names), 1),\n",
        "        palette\n",
        "    )\n",
        "    plt.imshow(full_color_map[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), label_names[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "LABEL_NAMES = np.asarray(classes)\n",
        "PALETTE = palette"
      ],
      "id": "vis_utils"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compare_litert_pt"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "np_images = []\n",
        "image_sizes = []\n",
        "\n",
        "for index in range(len(IMAGE_FILENAMES)):\n",
        "    # Retrieve each image from the file system.\n",
        "    image = Image.open(IMAGE_FILENAMES[index])\n",
        "    # Save the size for reference.\n",
        "    image_sizes.append(image.size)\n",
        "    # Convert each image into a NumPy array with shape (1, H, W, 3)\n",
        "    np_image = np.array(image.resize(MODEL_INPUT_HW, Image.Resampling.BILINEAR))\n",
        "    np_image = np.expand_dims(np_image, axis=0).astype(np.float32)\n",
        "    np_images.append(np_image)\n",
        "\n",
        "    # Retrieve an output from the converted model.\n",
        "    edge_model_output = edge_model(np_image)\n",
        "    segmentation_map = edge_model_output.squeeze()\n",
        "\n",
        "    # Visualize.\n",
        "    vis_segmentation(\n",
        "        np_images[index][0].astype(np.uint8),\n",
        "        np.argmax(segmentation_map, axis=-1),\n",
        "        PALETTE,\n",
        "        LABEL_NAMES\n",
        "    )"
      ],
      "id": "compare_litert_pt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize the LiteRT model.\n",
        "edge_model.export('segnext.tflite')"
      ],
      "metadata": {
        "id": "5rgUpVFInolB"
      },
      "id": "5rgUpVFInolB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk3Dd3ZZeey5"
      },
      "source": [
        "## Apply Quantization\n",
        "Model size matters on edge devices. Post-training quantization can significantly reduce the size of your `tflite` model. This section demonstrates how to use **dynamic-range quantization** through AI Edge Quantizer."
      ],
      "id": "kk3Dd3ZZeey5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGbRHy2jeey5"
      },
      "source": [
        "### Quantizing the model with dynamic quantization (AI Edge Quantizer)\n",
        "\n",
        "To use the `Quantizer`, we need to\n",
        "* Instantiate a Quantizer class. This is the entry point to the quantizer's functionalities.\n",
        "* Load a desired quantization recipe.\n",
        "* Quantize (and save) the model. This is where most of the quantizer's internal logic works."
      ],
      "id": "pGbRHy2jeey5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization (API will quantize and save a flatbuffer as *.tflite).\n",
        "quantizer = quantizer.Quantizer(float_model='segnext.tflite')\n",
        "quantizer.load_quantization_recipe(recipe=recipe.dynamic_wi8_afp32())\n",
        "\n",
        "quantization_result = quantizer.quantize()\n",
        "quantization_result.export_model('segnext_dynamic_wi8_afp32.tflite')"
      ],
      "metadata": {
        "id": "KgEnwA_-mvRS"
      },
      "id": "KgEnwA_-mvRS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`quantization_result` has two components\n",
        "\n",
        "\n",
        "* quantized LiteRT model (in bytearray) and\n",
        "* the corresponding quantization recipe"
      ],
      "metadata": {
        "id": "9EdSnuPsq8yn"
      },
      "id": "9EdSnuPsq8yn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the size of flatbuffers\n"
      ],
      "metadata": {
        "id": "AeXKNrJDq-wK"
      },
      "id": "AeXKNrJDq-wK"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.tflite"
      ],
      "metadata": {
        "id": "5W5rk99EnTZA"
      },
      "id": "5W5rk99EnTZA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what in this recipe\n",
        "\n"
      ],
      "metadata": {
        "id": "jljfKwytoQQa"
      },
      "id": "jljfKwytoQQa"
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_result.recipe"
      ],
      "metadata": {
        "id": "Pq9JjrMxoRWP"
      },
      "id": "Pq9JjrMxoRWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the recipe means: apply the naive min/max uniform algorithm (`min_max_uniform_quantize`) for all ops supported by the AI Edge Quantizer (indicated by `*`) under layers satisfying regex `.*` (i.e., all layers). We want the weights of these ops to be quantized as int8, symmetric, channel_wise, and we want to execute the ops in `Integer` mode."
      ],
      "metadata": {
        "id": "Y0Pm7OZ9odeZ"
      },
      "id": "Y0Pm7OZ9odeZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next, you'll create a function using LiteRT to run the newly generated quantized model.\n"
      ],
      "metadata": {
        "id": "ItdDEAZ7sm-v"
      },
      "id": "ItdDEAZ7sm-v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drq_convert",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @markdown We implemented some functions to run segmentation on the quantized model. <br/> Run the following cell to activate the functions.\n",
        "def run_segmentation(image, model_path):\n",
        "  \"\"\"Get segmentation mask of the image.\"\"\"\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  interpreter = ai_edge_litert.interpreter.Interpreter(model_path=model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  interpreter.set_tensor(input_details['index'], image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  output_details = interpreter.get_output_details()\n",
        "  output_index = 0\n",
        "  outputs = []\n",
        "  for detail in output_details:\n",
        "    outputs.append(interpreter.get_tensor(detail['index']))\n",
        "  mask = np.squeeze(outputs[output_index])\n",
        "  return mask"
      ],
      "id": "drq_convert"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let try running the newly quantized model and see how they compare."
      ],
      "metadata": {
        "id": "i8MDev7IrH1E"
      },
      "id": "i8MDev7IrH1E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drq_compare"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Validate the model.\n",
        "for index in range(len(IMAGE_FILENAMES)):\n",
        "    quantized_model_output = run_segmentation(np_images[index][0],\n",
        "                                             'segnext_dynamic_wi8_afp32.tflite')\n",
        "    vis_segmentation(\n",
        "        np_images[index][0].astype(np.uint8),\n",
        "        np.argmax(quantized_model_output, axis=-1),\n",
        "        PALETTE,\n",
        "        LABEL_NAMES\n",
        "    )"
      ],
      "id": "drq_compare"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrw5gdWLeey6"
      },
      "source": [
        "## Export and Download Models\n",
        "Let's save and download the converted `tflite` model, along with the dynamic-range quantized version."
      ],
      "id": "yrw5gdWLeey6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_models"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "files.download('segnext.tflite')"
      ],
      "id": "download_models"
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('segnext_dynamic_wi8_afp32.tflite')"
      ],
      "metadata": {
        "id": "DeowMFHIfke1"
      },
      "id": "DeowMFHIfke1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtUKRJQAeey6"
      },
      "source": [
        "## Next Steps\n",
        "Now you've got a fully converted (and optionally quantized!) `tflite` model. Here are some ideas on what to do next:\n",
        "\n",
        "- Explore [AI Edge Torch documentation](https://ai.google.dev/edge) for additional use cases or advanced topics.\n",
        "- Try out your new model on mobile or web using the [LiteRT API samples](https://ai.google.dev/edge/docs/litert).\n",
        "- Further tune or calibrate your quantization techniques to achieve the desired balance between model size and accuracy.\n",
        "\n",
        "Have fun deploying your model to the edge!"
      ],
      "id": "VtUKRJQAeey6"
    }
  ]
}