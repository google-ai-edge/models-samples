{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "Image_classification_with_convnext_v2.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 The AI Edge Torch Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "r4lisalb-A5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demo will teach you how to convert a PyTorch [ConvNext V2](https://huggingface.co/docs/transformers/en/model_doc/convnextv2#overview) model to a LiteRT (formally TensorFlow Lite) model using Google's AI Edge Torch library."
      ],
      "metadata": {
        "id": "LwrH6f2sGJ6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "Mzf2MdHoG-9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting the conversion process, ensure that you have all the necessary dependencies installed and required resources (like test images) available. You can start by importing the necessary dependencies for converting the model, as well as some additional utilities for displaying various information as you progress through this sample."
      ],
      "metadata": {
        "id": "hux_Gsc_G4nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ai-edge-torch-nightly\n",
        "!pip install transformers pillow requests matplotlib"
      ],
      "metadata": {
        "id": "l-9--DWON236"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will also need to download an image to verify model functionality."
      ],
      "metadata": {
        "id": "IUMh9GRk17fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['cat.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "  # TODO: Update path to the appropriate task subfolder in the GCS bucket\n",
        "  url = f'https://storage.googleapis.com/ai-edge/models-samples/torch_converter/image_classification_mobile_vit/{name}'\n",
        "  urllib.request.urlretrieve(url, name)"
      ],
      "metadata": {
        "id": "lfdgp-4Id51J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ],
      "metadata": {
        "id": "XYQeTVp-qqZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "# IMAGE_FILENAMES = list(uploaded.keys())\n",
        "\n",
        "# print('Uploaded files:', IMAGE_FILENAMES)"
      ],
      "metadata": {
        "id": "8X6tiqVGqq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now go ahead and verify that the image was loaded successfully"
      ],
      "metadata": {
        "id": "RIGfyYzcVkIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "\n",
        "# Preview the images.\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ],
      "metadata": {
        "id": "-GMYmZ5jVq6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch model validation"
      ],
      "metadata": {
        "id": "IBFYQIm-yFz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have your test images, it's time to validate the PyTorch model (in this case ConvNext V2) that will be converted to the LiteRT format.\n",
        "\n",
        "Start by retrieving the PyTorch model and the appropriate corresponding processor."
      ],
      "metadata": {
        "id": "g7qbJRCcvQJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ConvNextImageProcessor, ConvNextV2ForImageClassification\n",
        "\n",
        "# Define the Hugging Face model path\n",
        "hf_model_path = 'facebook/convnextv2-tiny-1k-224'\n",
        "\n",
        "# Initialize the image processor\n",
        "processor = ConvNextImageProcessor.from_pretrained(\n",
        "    hf_model_path\n",
        ")"
      ],
      "metadata": {
        "id": "flLiQaaL6tU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the image normalization parameters\n",
        "print(\"Image Mean:\", processor.image_mean)\n",
        "print(\"Image Std:\", processor.image_std)"
      ],
      "metadata": {
        "id": "RG1y5A2ifk_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_model = ConvNextV2ForImageClassification.from_pretrained(hf_model_path)"
      ],
      "metadata": {
        "id": "rh9oZFK2iydm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ConvNextImageProcessor` defined below will perform multiple steps on the input image to match the requirements of the `ConvNextV2` model:\n",
        "\n",
        "1. Rescale the image from the [0, 255] range to the range specified by the pretrained model.\n",
        "2. Resize input image to 224x224 pixels. Differes from default behaviour of processor (includes padding and center cropping) to make it easier to validate the converted model with LiteRT (more details in the corresponding section)."
      ],
      "metadata": {
        "id": "4Mik12qkNEU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "images = []\n",
        "for filename in IMAGE_FILENAMES:\n",
        "  images.append(Image.open(filename))\n",
        "\n",
        "inputs = processor(\n",
        "    images=images,\n",
        "    return_tensors='pt',\n",
        "    # Adjusts the image to have the shortest edge of 224 pixels\n",
        "    size={'shortest_edge': 224},\n",
        "    do_center_crop=False\n",
        ")"
      ],
      "metadata": {
        "id": "_-WmB2MYWc-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have your test data ready and the inputs processed, it's time to validate the classifications. In this step you will loop through your test image(s) and display the top 5 predicted classification categories. This model was trained with ImageNet-1000, so there are 1000 different potential classifications that may be applied to your test data."
      ],
      "metadata": {
        "id": "ZAQG5SSVzVi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "for image_index in range(len(IMAGE_FILENAMES)) :\n",
        "  outputs = pt_model(**inputs)\n",
        "  logits = outputs.logits\n",
        "  probs, indices = nn.functional.softmax(logits[image_index], dim=-1).flatten().topk(k=5)\n",
        "\n",
        "  print(IMAGE_FILENAMES[image_index], 'predictions: ')\n",
        "  for prediction_index in range(len(indices)):\n",
        "    class_label = pt_model.config.id2label[indices[prediction_index].item()]\n",
        "    prob = probs[prediction_index].item()\n",
        "    print(f'{(prob * 100):4.1f}%  {class_label}')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "ofbZW6nVzSrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to the `tflite` Format"
      ],
      "metadata": {
        "id": "pfJkS3bH7Jpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before converting the PyTorch model to work with the tflite format, you will need to take an extra step to match it to the format expected by LiteRT. Here are the necessary adjustments:\n",
        "\n",
        "1. **Channel Ordering**: Convert images from channel-first (BCHW) to channel-last (BHWC) format.\n",
        "2. **Softmax Layer**: Add a softmax layer to the classification logits as required by LiteRT as this is an image classification task.\n",
        "3. **Preprocessing Wrapper**: Incorporate preprocessing steps (e.g., RGB to BGR conversion, scaling, normalization) into a wrapper class, similar to what you did when validating the PyTorch model in the previous section.\n"
      ],
      "metadata": {
        "id": "ci-8lp_55TLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HF2LiteRT_ImageClassificationModelWrapper(nn.Module):\n",
        "\n",
        "  def __init__(self, hf_image_classification_model, hf_processor):\n",
        "    super().__init__()\n",
        "    self.model = hf_image_classification_model\n",
        "    if hf_processor.do_rescale:\n",
        "      self.rescale_factor = hf_processor.rescale_factor\n",
        "    else:\n",
        "      self.rescale_factor = 1.0\n",
        "\n",
        "    # Initialize image_mean and image_std as instance variables\n",
        "    self.image_mean = torch.tensor(hf_processor.image_mean).view(1, -1, 1, 1)  # Shape: [1, C, 1, 1]\n",
        "    self.image_std = torch.tensor(hf_processor.image_std).view(1, -1, 1, 1) # Shape: [1, C, 1, 1]\n",
        "\n",
        "  def forward(self, image: torch.Tensor):\n",
        "    # BHWC -> BCHW.\n",
        "    image = image.permute(0, 3, 1, 2)\n",
        "    # Scale [0, 255] -> [0, 1].\n",
        "    image = image * self.rescale_factor\n",
        "    # Normalize\n",
        "    image = (image - self.image_mean) / self.image_std\n",
        "    logits = self.model(pixel_values=image).logits  # [B, 1000] float32.\n",
        "    # Softmax is required for MediaPipe classification model.\n",
        "    logits = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "hf_convnext_v2_processor = ConvNextImageProcessor.from_pretrained(hf_model_path)\n",
        "hf_convnext_v2_model = ConvNextV2ForImageClassification.from_pretrained(hf_model_path)\n",
        "wrapped_pt_model = HF2LiteRT_ImageClassificationModelWrapper(\n",
        "    hf_convnext_v2_model, hf_convnext_v2_processor\n",
        ").eval()"
      ],
      "metadata": {
        "id": "NlBmvShe4Mt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to `TFLite`"
      ],
      "metadata": {
        "id": "GMBNfgcV7k0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to perform the conversion! You will need to provide simple arguments, such as the expected input shape (in this case three layers for images that are 224 height by 224 width)."
      ],
      "metadata": {
        "id": "T2MnULes70W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ai_edge_torch\n",
        "\n",
        "sample_args = (torch.rand((1, 224, 224, 3)),)\n",
        "edge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args)"
      ],
      "metadata": {
        "id": "XOfNPYpnLGrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the Converted Model\n",
        "\n",
        "Running the following saves the converted model as a **FlatBuffer** file, which is compatible with **LiteRT**.\n"
      ],
      "metadata": {
        "id": "yr6Lhls93tNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "TFLITE_MODEL_PATH = 'hf_convnext_v2_mp_image_classification_raw.tflite'\n",
        "flatbuffer_file = Path(TFLITE_MODEL_PATH)\n",
        "edge_model.export(flatbuffer_file)"
      ],
      "metadata": {
        "id": "cj-DW_2S4aUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate converted model with LiteRT"
      ],
      "metadata": {
        "id": "e7II2a_389DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to test your newly converted model directly with the LiteRT Interpreter API. Before getting into that code, you can add the following utility functions to improve the output displayed."
      ],
      "metadata": {
        "id": "-3kFtIGK_1qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Functions to visualize the image classification results. <br/> Run this cell to activate the functions.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams.update({\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.left': False,\n",
        "    'axes.spines.bottom': False,\n",
        "    'xtick.labelbottom': False,\n",
        "    'xtick.bottom': False,\n",
        "    'ytick.labelleft': False,\n",
        "    'ytick.left': False,\n",
        "    'xtick.labeltop': False,\n",
        "    'xtick.top': False,\n",
        "    'ytick.labelright': False,\n",
        "    'ytick.right': False\n",
        "})\n",
        "\n",
        "\n",
        "def display_one_image(image, title, subplot, titlesize=16):\n",
        "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "\n",
        "def display_batch_of_images(images, predictions):\n",
        "    \"\"\"Displays a batch of images with the classifications.\"\"\"\n",
        "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images) // rows\n",
        "\n",
        "    # Size and spacing.\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols, 1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "\n",
        "    # Display.\n",
        "    for i, (image, prediction) in enumerate(zip(images[:rows*cols], predictions[:rows*cols])):\n",
        "        dynamic_titlesize = FIGSIZE * SPACING / max(rows,cols) * 40 + 3\n",
        "        subplot = display_one_image(image, prediction, subplot, titlesize=dynamic_titlesize)\n",
        "\n",
        "    # Layout.\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mV2Uo2yg2Lw4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with LiteRT Interpreter\n",
        "\n",
        "Now it's time to move on to the actual inference code and display the highest confidence classification result. Let's now run inference using the converted LiteRT model and compare the results with the original PyTorch model."
      ],
      "metadata": {
        "id": "5_HCsDSI3cOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the LiteRT model and allocate tensors.\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "\n",
        "# Path to the converted LiteRT model\n",
        "TFLITE_MODEL_PATH = 'hf_convnext_v2_mp_image_classification_raw.tflite'\n",
        "\n",
        "# Initialize the LiteRT interpreter\n",
        "interpreter = Interpreter(TFLITE_MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"LiteRT Model Input Details:\", input_details)\n",
        "print(\"LiteRT Model Output Details:\", output_details)"
      ],
      "metadata": {
        "id": "IuHqd7414MbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Preprocessing and Postprocessing Functions\n",
        "Prepare functions to preprocess images for LiteRT and to extract top predictions."
      ],
      "metadata": {
        "id": "u34Rw0VX3NAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_lite(image_path, size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Loads an image, resizes it to the specified size, and converts it to a NumPy array.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_resized = image.resize(size, Image.Resampling.BILINEAR)\n",
        "    image_array = np.array(image_resized).astype(np.float32)\n",
        "    # Expand dimensions to match model's expected input shape (1, H, W, C)\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "    return image_array\n",
        "\n",
        "def get_top_k_predictions_lite(output, k=5):\n",
        "    \"\"\"\n",
        "    Returns the top K predictions from the already softmaxed output.\n",
        "    \"\"\"\n",
        "    # Convert the numpy array to a PyTorch tensor\n",
        "    probs_tensor = torch.from_numpy(output)\n",
        "\n",
        "    # Retrieve the top K probabilities and their corresponding indices\n",
        "    top_probs, top_indices = torch.topk(probs_tensor, k)\n",
        "\n",
        "    # Convert the results back to numpy arrays and flatten them\n",
        "    return top_probs.numpy().flatten(), top_indices.numpy().flatten()"
      ],
      "metadata": {
        "id": "vvhbSOHM5NNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference and Visualize\n",
        "Execute the inference process and visualize the predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ko1LHnoW3Efv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "predictions = []\n",
        "\n",
        "for image_name in IMAGE_FILENAMES:\n",
        "    # STEP 1: Load the input image(s).\n",
        "    image = np.array(Image.open(image_name).convert('RGB'))\n",
        "\n",
        "    # STEP 2: Load and preprocess the input image\n",
        "    lite_input = preprocess_image_lite(image_name, size=(224, 224))\n",
        "\n",
        "    # STEP 3: Classify the input image using LiteRT model\n",
        "    interpreter.set_tensor(input_details[0]['index'], lite_input)\n",
        "    interpreter.invoke()\n",
        "    lite_output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # STEP 4: Process the classification result (get top 5 predictions)\n",
        "    lite_probs, lite_indices = get_top_k_predictions_lite(lite_output, k=5)\n",
        "\n",
        "    # STEP 5: Get the top category (highest probability) and visualize\n",
        "    top_prob = lite_probs[0]\n",
        "    top_idx = lite_indices[0]\n",
        "    top_category_name = pt_model.config.id2label[top_idx]\n",
        "    prediction_text = f\"{top_category_name} ({top_prob * 100:.2f}%)\"\n",
        "\n",
        "    images.append(image)\n",
        "    predictions.append(prediction_text)\n",
        "\n",
        "# Display the image with prediction\n",
        "display_batch_of_images(images, predictions)"
      ],
      "metadata": {
        "id": "4GdTsCQP10To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now see your loaded test images and their confidence scores/classifications that match the original PyTorch model results! If everything looks good, the final step should be downloading your newly converted `tflite` model file to your computer so you can use it elsewhere."
      ],
      "metadata": {
        "id": "1bxosFdH_99n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(TFLITE_MODEL_PATH)"
      ],
      "metadata": {
        "id": "mY00XJQ1BZP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps\n",
        "\n",
        "Now that you have learned how to convert a PyTorch model to the LiteRT format, it's time to check out the [LiteRT Interpreter API](https://ai.google.dev/edge/litert) for running other custom solutions, and read more about the PyTorch to LiteRT framework with our [official documentation](https://ai.google.dev/edge/lite/models/convert_pytorch)."
      ],
      "metadata": {
        "id": "_6BEXn642zBu"
      }
    }
  ]
}